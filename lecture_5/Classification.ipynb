{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#импортируем необходимые модули\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-7fb073810db8>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-7fb073810db8>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    def accuracy(y_hat, y, thresh):\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def recall(y_hat, y, thresh):#y_hat-вероятность, tresh - граница, по которой нужно делить, классифицировать, y - наши д\n",
    "    return sum((((y-1)*2)+1)==(y_hat>trash))/sum(y)#все единички в нем, см формулу\n",
    "def precision(y_hat, y, thresh):\n",
    "    #TODO\n",
    "    \n",
    "def accuracy(y_hat, y, thresh):\n",
    "\n",
    "    \n",
    "def f1(y_hat, y, thresh):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 passed\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'recall' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f7019d467531>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mthresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_hat\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_hat\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_hat\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'recall' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Test {} passed\".format(i))\n",
    "    y_hat = np.random.random(30)\n",
    "    y = np.random.random(30) > 0.7\n",
    "    thresh = np.random.random()\n",
    "    \n",
    "    assert abs(recall(y_hat, y, thresh) - recall_score(y_true=y, y_pred=y_hat > thresh)) < 0.1\n",
    "    assert abs(precision(y_hat, y, thresh) - precision_score(y_true=y, y_pred=y_hat > thresh)) < 0.1\n",
    "    assert abs(accuracy(y_hat, y, thresh) - accuracy_score(y_true=y, y_pred=y_hat > thresh)) < 0.1\n",
    "    assert abs(f1(y_hat, y, thresh) - f1_score(y_true=y, y_pred=y_hat > thresh)) < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = (1,0) #TODO предсказания, чтобы выбить recall 0.9 плохим классификатором\n",
    "y = (1,1) #TODO метки, чтобы выбить recall 0.9 плохим классификатором\n",
    "#t = #TODO порог, чтобы выбить recall 0.9 плохим классификатором\n",
    "\n",
    "assert recall_score(y_hat, y) > 0.9\n",
    "\n",
    "y_hat = (1,1,1) #TODO предсказания, чтобы выбить precision 0.9 плохим классификатором\n",
    "y = (1,1,1) #TODO метки, чтобы выбить precision 0.9 плохим классификатором\n",
    "#t = #TODO порог, чтобы выбить precision 0.9 плохим классификатором\n",
    "\n",
    "assert precision_score(y_hat, y) > 0.9\n",
    "\n",
    "y_hat =(1,1,1) #TODO предсказания, чтобы выбить accuracy 0.9 плохим классификатором\n",
    "y = (1,1,1)#TODO метки, чтобы выбить accuracy 0.9 плохим классификатором\n",
    "#t = #TODO порог, чтобы выбить accuracy 0.9 плохим классификатором\n",
    "\n",
    "assert accuracy_score(y_hat, y) > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь для логистической регрессии (из материалов курса по нейронным сетям)\n",
    "\n",
    "Давайте получим функцию потерь аналогичным образом, как мы это сделали с линейной регрессией.\n",
    "\n",
    "\n",
    "логарифм правдоподобия\n",
    "$$\n",
    "\\large \n",
    "\\begin{array}{rcl} \n",
    "    \\log P\\left(\\vec{y} \\mid X, \\vec{w}\\right) \n",
    "        &=& \\log \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right) \\\\ \n",
    "        &=& \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\vec{w}^T\\vec{x_i}) \\\\ \n",
    "        &=& \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\vec{w}^T\\vec{x_i}) \\\\ \n",
    "        &=& \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}} \\\\ \n",
    "        &=& - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}) \n",
    "\\end{array}\n",
    "$$\n",
    "который и будем оптимизировать. \n",
    "\n",
    "Данная функция потерь \n",
    "$$\n",
    "\\large L(\\vec{x},\\vec{y},\\vec{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}})\n",
    "$$\n",
    "называется логистичской функцией потерь или сокращенно logloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано:\n",
    "- X - обучающее множество\n",
    "- Y - метки классов для обучающего множества\n",
    "- $\\eta$ - скорость обучения\n",
    "\n",
    "Алгоритм:\n",
    "1. инициализируем $\\vec{w}_0$ небольшими случайными значениями\n",
    "2. выбираем случайный объект $x_i$ с меткой $y_i$\n",
    "3. обновляем веса $ \\vec{w}_{t+1} = \\vec{w}_{t} - \\eta \\nabla L $\n",
    "4. если условие остановки не выполнело, возвращаемся к шагу 2\n",
    "\n",
    "Выпишем градиент функции потерь:\n",
    "$$\n",
    "    \\nabla L = \\left\\{ - \\frac{y_i \\vec{x_i} }  { 1 + \\exp^{y_i\\vec{w}^T\\vec{x_i}} } + \\lambda \\vec{w} \\right\\} \n",
    "$$\n",
    "\n",
    "На практике, в качестве условия остановки, часто используют изменение функции потерь на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "selector = iris.target != 0 # ограничемся только двумя классами\n",
    "X = iris.data[:, [1,3]][selector]  # и двумя переменными\n",
    "y = iris.target[selector]\n",
    "y[y==2] = 0 # конвертируем метки классов к 1/-1\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[:80]\n",
    "X_test = X[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polytrans = PolynomialFeatures(3)\n",
    "X_train = polytrans.fit_transform(X_train)\n",
    "X_test = polytrans.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_1 = #TODO ОБучить логистическую регрессию на train с C=0.1\n",
    "pred_1 = #TODO Предсказать логистической регрессией на трейне\n",
    "pred_test = #TODO Предсказать логистической регрессией на тесте\n",
    "#TODO Сранивть метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO обучим логистическую регрессию с разными C и посмотрим как меняются метрики на трейне и на тесте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример текстовой классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Анна\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Анна\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # сколько встретилось в нашем тексте\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import grid_search\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0          2 . take around 10,000 640x480 pictures .  1\n",
       "1  i downloaded a trial version of computer assoc...  1\n",
       "2  the wrt54g plus the hga7t is a perfect solutio...  1\n",
       "3  i dont especially like how music files are uns...  0\n",
       "4  i was using the cheapie pail ... and it worked...  1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('products_sentiment_train.tsv', sep='\\t', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Label\n",
       "0          2 . take around 10,000 640x480 pictures .      1\n",
       "1  i downloaded a trial version of computer assoc...      1\n",
       "2  the wrt54g plus the hga7t is a perfect solutio...      1\n",
       "3  i dont especially like how music files are uns...      0\n",
       "4  i was using the cheapie pail ... and it worked...      1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['Review', 'Label']#отзыв, полотр\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-afa2b6305fb3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-43-afa2b6305fb3>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    data['Review'].apply(lambda x: \"\".join(for i in x if i not in exclude))\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data['Review'].apply(lambda x: \"\".join(for i in x if i not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer().fit(data['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = vectorizer.transform.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "for i in range(0,len(data)):\n",
    "    arr = data.iloc[i,0].split(' ')\n",
    "    data.iloc[i,0] = ' '.join([str(porter_stemmer.stem(wordnet_lemmatizer.lemmatize(arr[i]))) for i in range(0, len(arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = [CountVectorizer(), TfidfVectorizer()]\n",
    "parameters_grid = {\n",
    "    'vectorizer__stop_words' : [nltk.corpus.stopwords.words('english'), 'english', None],\n",
    "    'vectorizer__ngram_range' : [(1,2), (1,3)],\n",
    "    'classifier__C':[0.25,0.5,1,2,5,10,15],\n",
    "    'classifier__class_weight':['balanced', None],\n",
    "    'classifier__max_iter':[70,100,200],\n",
    "    'classifier__penalty':['l2','l1']\n",
    "}\n",
    "for vect in vectors:\n",
    "        pipeline = Pipeline([('vectorizer', vect),('classifier',LogisticRegression())])\n",
    "        grid_cv = grid_search.GridSearchCV(pipeline, parameters_grid, n_jobs=-1, scoring = 'accuracy', cv = 3)\n",
    "        grid_cv.fit(data[0],data[1])\n",
    "        print('Classifier: logReg, vectoizer: ',vect,', best_score: ',grid_cv.best_score_, grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('products_sentiment_test.tsv', sep='\\t')\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(data_test)):\n",
    "    arr = data_test.iloc[i,1].split(' ')\n",
    "    data_test.iloc[i,1] = ' '.join([str(porter_stemmer.stem(wordnet_lemmatizer.lemmatize(arr[i].lower()))) for i in range(0, len(arr))])\n",
    "res = clf.predict(vectorizer.transform(data_test.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Appendix TODO together. Классификация отзывов о технике на русском"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
